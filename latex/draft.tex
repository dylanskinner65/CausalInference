\documentclass{article}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{ntheorem}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\usepackage{tkz-graph}
\usetikzlibrary{arrows}
\usetikzlibrary{shapes.geometric}


\SetVertexNormal[Shape      = circle,
                 FillColor  = cyan,
                 LineWidth  = 2pt]
\SetUpEdge[lw         = 1pt,
           color      = black,
           labelcolor = white,
           labeltext  = red,
           labelstyle = {sloped,draw,text=blue}]
\tikzset{Bullet/.style={fill=black,draw,color=#1,circle,minimum size=3pt,scale=0.75}}


\begin{document}


\title{Causal Inference with Graphical Neural Networks}
\author{Gwen Johnson\and Dylan Skinner\and Dallin Stewart\and Jason Vasquez}
\date{\today}
\maketitle

\section{Introduction}


\section{Graph Neural Networks}

Originally proposed in 2005 by Mori et al.~\cite{gori2005new}, Graph Neural Networks 
(GNNs) are a class of neural networks that operate on graph-structured data. 
They have gained significant attention in recent years due to their versatility 
in handling various types of graph data, including social networks, citation networks, 
biological networks, and more. 

Unlike traditional neural networks, which operate on 
grid-like structures such as images or sequences, GNNs are specifically designed to 
capture and leverage the structural information present in graphs. One of their 
key strengths lies in their ability to learn meaningful representations of nodes in a 
graph, which can then be used for various downstream tasks such as node classification, 
link prediction, and graph classification. Their ability to capture and model complex 
relationships in graph data makes them invaluable tools for exploring and understanding 
real-world phenomena represented in graph form.

In our project, we are utilizing the power of GNNs to perform a supervised classification
task. This means we are training a GNN on a labeled, tabular dataset, where each graph instance
(or node) is associated with a target label.


% Introduce the topic of causal inference with Graphical Neural Networks (GNNs).

\section{$\textbf{do}$ Operator}

The \textbf{do} operator is a way to represent interventions in a causal model. 
It is a way to represent the effect of an intervention on a variable. As an example, 
consider the following model involving smoking.

If a person's fingernails $(N)$ have turned yellow, this implies a higher probability 
that they are a heavy smoker $(S)$ and hence have a higher probability of developing 
lung cancer $(C)$. But, simply dyeing a persons fingernails yellow does not impact 
their probability of developing lung cancer. 

So, in terms of $\textbf{do}$ calculus, we can denote the process of setting a 
variable $N$ to have a value $\textit{yellow}$ by $\textbf{do}(N = \textit{yellow})$. 
We note that 

\begin{equation*}
P(C \;|\;N = \textit{yellow}) \neq P(C\;|\; \textbf{do}(N=\textit{yellow})).
\end{equation*}

With this in mind, we now define the $\textbf{do}$ operator.

\begin{theorem}[{{\cite{pearl2009causal}}}]
    In a causal diagram $\Gamma$ with nodes $X_1,\dots, X_n$ and joint distribution 
    $P(X_1, \dots, X_n)$, the result of doing $X_i = x_i$ on the joint distribution is

    \[
        P(X_1, \dots, X_n \;|\; \textbf{do}(X_i = x_i)) = \frac{P(x_1,\dots,x_n)}{P(x_i\;|\; \textup{par}(x_i))} = \prod_{j\neq i}P(x_j\;|\; \textup{par}(x_j)).
    \]
    \label{theorem:do}
\end{theorem}

In this, we have $\text{par}(x_i)$ represent values of the parent nodes of $\text{PAR}(X_i)$ of $X_i$ in $\Gamma$.
The probabilities on the right hand side of the above equation are what we call \textit{preintervention}. 
This means they use the original probabilities from the original model before doing $X_i = x_i$.

It is important to note that the above equation is how we calculate the probability of several events 
happening given one event has happened. What if we want to get the probability of a single event happening, 
given we do a single event? That leads to the following corollary.

\begin{corollary}
    If $X$ and $Y$ are random variables in a causal diagram $\Gamma$ and $\textup{PAR}(X)$ are the parents of $X$, then

    \[P(y\;|\;  \textbf{do}(x)) = \sum_{\textup{par}}\frac{P(x,\,y,\,\textup{par})}{P(x\;|\; \textup{par})},\]

    where the sum runs over all values $\textup{par}$ that the variables $\textup{PAR}(X)$ can take. If $X$ has no parents, then

    \[P(y\;|\; \textbf{do}(x)) = \frac{P(x,\,y)}{P(x)} = P(y\;|\; x).\]
    \label{corollary:do}
\end{corollary}

Let us now consider a basic example to see how this works. Consider the following causal diagram in Figure~\ref{fig:causal_dia}.

\begin{figure}[h]
    \centering
    \begin{tikzpicture}[scale=1.5, Bullet/.style={circle, draw=black, fill=#1, inner sep=0pt, minimum size=3mm, line width=1.pt}]
        \node[Bullet=cyan,label=above :{$A$}] (A) at (0,2){};
        \node[Bullet=cyan,label=above :{$C$}] (C) at (4,2){};
        \node[Bullet=cyan,label=below :{$B$}] (B) at (2,1){};
        \node[Bullet=cyan,label=below :{$Y$}] (Y) at (4,0){};
        \node[Bullet=cyan,label=below :{$X$}] (X) at (0,0){};
    
        \draw[->, line width=1] (A) -- (B);
        \draw[->, line width=1] (C) -- (B);
        \draw[->, line width=1] (B) -- (X);
        \draw[->, line width=1] (C) -- (Y);
        \draw[->, line width=1] (A) -- (X);
    \end{tikzpicture}
    \caption{Basic causal diagram. Note it is in the form of a directed acyclic graph (DAG).}
    \label{fig:causal_dia}
\end{figure}

In this diagram, we can see that $A$ and $C$ are both parents of $B$. So, for any values of $x$ and $b$, 
Corollary~\ref{corollary:do} tells us that

\begin{align*}
    P(X = x \,|\, \textbf{do}(B=b)) &= \sum_{\text{par}(b)}\frac{P(x, \,b, \,\text{par}(b))}{P(b\,|\, \text{par}(b))} \\
    %  &= \sum_{a}\sum_{c}\frac{P(X=x\,, \, A=a\,,\, B=b \, ,\, C=c)}{P(B=b \;|\; A=a,\, C=c)} \\
    %  &= \sum_{a}\sum_{c}\frac{P(X=x\;|\; A=a\,,\, B=b)P(B=b\;|\; A=a,\, C=c)P(A=a)P(C=c)}{P(B=b \;|\; A=a,\, C=c)} \\
    %  &= \sum_{a}\sum_{c}P(X=x\;|\; A=a\,,\, B=b)P(A=a)P(C=c) \\
\end{align*}

which, written out, is 

\begin{equation*}
    \sum_{\text{par}(b)}\frac{P(x, \,b, \,\text{par}(b))}{P(b\,|\, \text{par}(b))} = \sum_{a}\sum_{c}\frac{P(X=x\,, \, A=a\,,\, B=b \, ,\, C=c)}{P(B=b \;|\; A=a,\, C=c)}.
\end{equation*}

By dependence of nodes only on their parents and the rules of probability, this turns into

\begin{equation*}
    \sum_{a}\sum_{c}\frac{P(X=x\;|\; A=a\,,\, B=b)P(B=b\;|\; A=a,\, C=c)P(A=a)P(C=c)}{P(B=b \;|\; A=a,\, C=c)},
\end{equation*}

which simplifies to

\begin{equation*}
    \sum_{a}\sum_{c}P(X=x\;|\; A=a\,,\, B=b)P(A=a)P(C=c).
\end{equation*}

Since there is only one instance where we are considering the probability with respect to $c$, 
we can simplify this to

\begin{equation*}
    \sum_{a}P(X=x\;|\; A=a\,,\, B=b)P(A=a),
\end{equation*}

which is our final answer.

While this introduction to the \textbf{do} operator might feel a bit abstract, it is the foundation
of all current research in causal inference. 





\section{Data}

For our project, we used the LUCAS0 dataset~\cite{lucas_dataset}, which is a toy data set 
generated artificially by causal Bayesian networks with binary variables. The LUCAS0 dataset
is a DAG with 11 nodes and 2000 training samples, where the DAG is represented as in Figure~\ref{fig:lucas_dag}.

\begin{figure}[h]
    \centering
    \begin{tikzpicture}[scale=1.5, Bullet/.style={circle, draw=black, fill=#1, inner sep=0pt, minimum size=3mm, line width=1.pt}]
        % \draw[help lines, gray] (0,0) grid (6,6);
        \node[ellipse, draw=black, fill=lime] (A) at (2,2){Anxiety};
        \node[ellipse, draw=black, fill=lime, align=center] (P) at (4,2){Peer\\Pressure};
        \node[ellipse, draw=black, fill=teal] (S) at (3,1){Smoking};
        \node[ellipse, draw=black, fill=lime, align=center] (Y) at (1,1){Yellow\\Fingers};
        \node[ellipse, draw=black, fill=lightgray, align=center] (B) at (6, 2){Born on\\Even Day};
        \node[ellipse, draw=black, fill=teal, align=center] (G) at (5, 1){Genetics};
        \node[ellipse, draw=black, fill=teal, align=center] (Al) at (2, 0){Allergy};
        \node[ellipse, draw=black, fill=pink, align=center] (L) at (4, 0){Lung\\Cancer};
        \node[ellipse, draw=black, fill=lime, align=center] (AD) at (6, 0){Attention\\Disorder};
        \node[ellipse, draw=black, fill=teal, align=center] (C) at (3, -1){Coughing};
        \node[ellipse, draw=black, fill=teal, align=center] (F) at (5, -1){Fatigue};
        \node[ellipse, draw=black, fill=lime, align=center] (CA) at (6, -2){Car\\Accident};
    
        \draw[->, line width=1] (A) -- (S);
        \draw[->, line width=1] (P) -- (S);
        \draw[->, line width=1] (S) -- (Y);
        \draw[->, line width=1] (S) -- (L);
        \draw[->, line width=1] (G) -- (L);
        \draw[->, line width=1] (G) -- (AD);
        \draw[->, line width=1] (Al) -- (C);
        \draw[->, line width=1] (L) -- (C);
        \draw[->, line width=1] (L) -- (F);
        \draw[->, line width=1] (C) -- (F);
        \draw[->, line width=1] (F) -- (CA);
        \draw[->, line width=1] (AD) -- (CA);

    \end{tikzpicture}
    \caption{Basic causal diagram. Note it is in the form of a directed acyclic graph (DAG). Our target variable
    is shaded in ${\color{pink} \textbf{pink}}$, and the nodes in {\color{teal} \textbf{teal}} constitude the Markov
    blanket of the target variable.}
    \label{fig:lucas_dag}
\end{figure}






\section{Background}

% Provide background information on causal inference, neural networks, and Graphical Neural Networks (GNNs).

\section{Methodology}

% Describe the methodology of using GNNs for causal inference. Include information on how GNNs are trained, how causal inference is performed, etc.

\section{Experiments}

% Present the experiments conducted to evaluate the performance of GNNs for causal inference. Include dataset descriptions, experimental setup, and results.

\subsection{Dataset Description}

% Describe the datasets used in the experiments.

\subsection{Experimental Setup}

% Explain the experimental setup, including model architecture, training procedure, hyperparameters, etc.

\subsection{Results}

% Present and analyze the results of the experiments. Include metrics, comparisons with baseline methods, and any insights gained.

\section{Discussion}

% Discuss the implications of the results, limitations of the approach, potential future research directions, etc.

\section{Conclusion}

% Summarize the key findings of the study and conclude the paper.

\section*{Acknowledgments}

% Acknowledge any individuals or organizations that contributed to the project.


\newpage
% Bibliography
\bibliographystyle{plain}
\bibliography{references} % Replace 'references' with the name of your .bib file

\end{document}
